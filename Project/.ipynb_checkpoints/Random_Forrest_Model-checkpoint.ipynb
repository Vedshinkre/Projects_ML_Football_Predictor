{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddf4d1e-fa2a-43ff-ab10-f90fd08d9396",
   "metadata": {},
   "source": [
    "# Predicting Football Match Outcomes Using Machine Learning  \n",
    "\n",
    "In this project, we will explore various **methods and predictors** to enhance the accuracy of our football match outcome predictions. Our approach is structured into **three distinct groups of predictors**, each progressively incorporating more detailed insights into team performance and rankings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63a0e3-3f5b-4a70-ac33-01f4734c89ac",
   "metadata": {},
   "source": [
    "# Decision Trees and Their Application in Classification  \n",
    "\n",
    "In this section, we will **test** and **evaluate** various tree-based methods, including **classification trees**, **bagging**, **random forest**, and **boosting**, to see which works best for predicting football match outcomes.\n",
    "\n",
    "## What Are Decision Trees?  \n",
    "\n",
    "A **decision tree** is a supervised machine learning algorithm used for both **classification** and **regression** tasks. It splits the data into subsets based on the most significant feature at each step, resulting in a tree-like structure. \n",
    "\n",
    "### How Do Decision Trees Work for Classification?  \n",
    "\n",
    "In **classification tasks**, decision trees are used to predict categorical outcomes by recursively splitting the data at each node, with the goal of maximizing the \"purity\" of the resulting subsets. The **root node** represents the entire dataset, and the tree branches out to **leaf nodes** that represent the predicted class label. Each split is based on the feature that best separates the data at that point, typically using a metric such as **Gini impurity** or **cross entropy**.\n",
    "\n",
    "#### Example Process:\n",
    "1. **Starting Node (Root)**: The algorithm evaluates all possible features and chooses the one that best divides the data into distinct classes.\n",
    "2. **Internal Nodes**: Each subsequent node splits the data based on a feature that provides the greatest separation of class labels.\n",
    "3. **Leaf Nodes**: These represent the final predicted class labels for a given subset of data.\n",
    "\n",
    "Decision trees are **easy to interpret** and visualize, which makes them an appealing choice for understanding how predictions are made. However, they can be prone to **overfitting** if not properly tuned.\n",
    "\n",
    "## Types of Tree-Based Methods  \n",
    "\n",
    "#### 1. Classification Trees  \n",
    "#### 2. Bagging (Bootstrap Aggregating)  \n",
    "#### 3. Random Forest  \n",
    "#### 4. Boosting  \n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this section, we have explored various tree-based methods—**classification trees**, **bagging**, **random forest**, and **boosting**—and will test each one to determine which best suits the football match outcome prediction task. Each method will be tested and evaluated to assess its effectiveness in predicting match results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "02986de0-943a-4fd7-854d-5a7592f086d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24047133-9169-4b73-b0d9-a9b64e4499dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "df45c223-33d1-42f6-a0e4-78827eeca076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0b5262-a4aa-42f0-bbf3-6d5d1ee33377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "matches = pd.read_csv(\"premierleague_team_data.csv\")\n",
    "\n",
    "#loading the testing data \n",
    "test_matches = pd.read_csv(\"premierleague_test_team_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "205bf85c-4971-4ab3-98f1-e129ecc2b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "new_matches = pd.read_csv(\"premierleague_rank_team_data.csv\")\n",
    "\n",
    "#loading the testing data with rank\n",
    "new_test_matches = pd.read_csv(\"premierleague_rank_test_team_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd8a1a8b-d578-4e24-a921-c10bb2f2a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "91fb9b7a-d070-4163-8148-30ad95742c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ee9fb-30cf-4fa6-951c-211798f5ce25",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "\n",
    "A **Classification Tree** is a type of decision tree used for classification tasks. It works by recursively splitting the dataset into subsets based on feature values, aiming to maximize the separation between different classes. The final result is a tree-like model where each leaf node represents a class label.\n",
    "\n",
    "## How It Works:\n",
    "- **Recursive Partitioning**: The dataset is split into smaller groups using the most informative features.\n",
    "- **Gini Impurity / Entropy**: The quality of splits is determined using metrics like Gini Impurity or Entropy.\n",
    "- **Tree Growth**: The process continues until a stopping criterion is met (e.g., maximum depth, minimum samples per split).\n",
    "- **Prediction**: For a new input, the model follows the decision path and assigns a class label based on the majority vote in the final node.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Easy to Interpret**: The decision-making process is visual and intuitive.  \n",
    "✅ **Requires Minimal Data Preprocessing**: No need for feature scaling or normalization.  \n",
    "✅ **Captures Non-Linear Relationships**: Works well with complex decision boundaries.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Prone to Overfitting**: Without pruning, the tree can become too complex and fit noise in the data.  \n",
    "❌ **Unstable**: Small changes in data can result in a significantly different tree.  \n",
    "❌ **Less Accurate Than Ensembles**: Single decision trees are often outperformed by ensemble methods like Random Forests and Gradient Boosting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9d0a3-729b-4a98-acf4-f688b5cb3ae0",
   "metadata": {},
   "source": [
    "## **Pruning in Classification Tree**  \n",
    "Pruning helps prevent **overfitting** by reducing the size of a decision tree, leading to improved accuracy on unseen data. Without pruning, a tree may **memorize** training data rather than generalizing well to new data.  \n",
    "\n",
    "### **Post-Pruning (Cost Complexity Pruning - CCP)**  \n",
    "In post-pruning, the tree is first grown to full depth (even if it overfits) and then gradually pruned by removing nodes based on a complexity parameter α .  \n",
    "\n",
    "#### **How CCP Works?**  \n",
    "The pruning process minimizes the following equation:  \n",
    "\n",
    "$$\n",
    "\\text{Total Cost} = \\text{RSS} + \\alpha \\times \\text{Number of Leaves}\n",
    "$$\n",
    "\n",
    "\n",
    "- **RSS (Residual Sum of Squares)** measures the error in predictions.  \n",
    "- **α** is a tuning parameter that controls the trade-off between tree complexity and error.  \n",
    "  - **Higher α** → More pruning → Simpler tree.  \n",
    "  - **Lower α** → Less pruning → More complex tree.  \n",
    "- The value for **α** can be found using cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7d658595-05b2-4ffe-a4ac-4838901113a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find optimal ccp_alpha\n",
    "def find_optimal_alpha(Train):\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    \n",
    "    dt = DecisionTreeClassifier(random_state=1)\n",
    "    path = dt.cost_complexity_pruning_path(Train[static_predictors], Train[\"Target\"])\n",
    "    ccp_alphas = path.ccp_alphas[:-1]  # Exclude the last value to avoid a single-node tree\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    alpha_scores = {}\n",
    "    \n",
    "    for alpha in ccp_alphas:\n",
    "        dt = DecisionTreeClassifier(random_state=1, ccp_alpha=alpha)\n",
    "        scores = cross_val_score(dt, Train[static_predictors], Train[\"Target\"], cv=kf, scoring='accuracy')\n",
    "        alpha_scores[alpha] = np.mean(scores)\n",
    "    \n",
    "    best_alpha = max(alpha_scores, key=alpha_scores.get)\n",
    "    print(f\"Best ccp_alpha: {best_alpha:.6f} with Accuracy: {alpha_scores[best_alpha]:.4f}\")\n",
    "    return best_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b06ef3-dd4e-4d6b-9f5b-ac3eabe8831a",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors  (refer Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8ffe34e5-22a1-4755-9b0c-2845846393ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions\n",
    "def make_yearly_predictions_decs(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "            # Train Decision Tree with externally provided ccp_alpha\n",
    "            dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            dt.fit(Train[static_predictors], Train[\"Target\"])\n",
    "            preds = dt.predict(test_year[static_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c911a6e9-98d7-4bbd-93dd-972585395c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2013 with 380 matches.\n",
      "Year 2013: Precision = 0.6236, Accuracy = 0.6421\n",
      "\n",
      "Testing on year: 2014 with 760 matches.\n",
      "Year 2014: Precision = 0.6508, Accuracy = 0.6592\n",
      "\n",
      "Testing on year: 2015 with 760 matches.\n",
      "Year 2015: Precision = 0.6326, Accuracy = 0.6513\n",
      "\n",
      "Testing on year: 2016 with 756 matches.\n",
      "Year 2016: Precision = 0.6612, Accuracy = 0.6720\n",
      "\n",
      "Testing on year: 2017 with 802 matches.\n",
      "Year 2017: Precision = 0.6351, Accuracy = 0.6521\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.5975, Accuracy = 0.6257\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "make_yearly_predictions_decs(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7e2f8abe-9623-45d1-b016-610a30b1ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 398 matches.\n",
      "Year 2019: Precision = 0.5688, Accuracy = 0.5905\n",
      "\n",
      "Testing on year: 2020 with 672 matches.\n",
      "Year 2020: Precision = 0.5826, Accuracy = 0.5967\n",
      "\n",
      "Testing on year: 2021 with 816 matches.\n",
      "Year 2021: Precision = 0.5688, Accuracy = 0.5870\n",
      "\n",
      "Testing on year: 2022 with 722 matches.\n",
      "Year 2022: Precision = 0.5871, Accuracy = 0.6025\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.5891, Accuracy = 0.6042\n"
     ]
    }
   ],
   "source": [
    "# Testing Precision and Accuracy\n",
    "make_yearly_predictions_decs(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e03cb-ff95-42d7-b461-7567dd50dd42",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors + Rolling Predictors (refer Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bbc3ce7a-2b4f-4562-b70b-20122e4688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "            # Train Decision Tree with externally provided ccp_alpha\n",
    "            dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "            preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "74cbde2e-6586-423d-a7d6-14e07cba5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.7182, Accuracy = 0.7250\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.6663, Accuracy = 0.6751\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.7188, Accuracy = 0.7251\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.7149, Accuracy = 0.7211\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.6932, Accuracy = 0.7018\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_decs_rolling(matches, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "06a39df1-0c90-4a6e-95ad-0153112f981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.5552, Accuracy = 0.5710\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.5949, Accuracy = 0.6139\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.5703, Accuracy = 0.5904\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6148, Accuracy = 0.6287\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6233, Accuracy = 0.6389\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_decs_rolling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51a343-7eb3-4fe7-86e4-d26a8dc9a201",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Full Feature Set (refer Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "656e91e7-0320-436a-8cbc-69ff91ab5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors =  [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "            # Train Decision Tree with externally provided ccp_alpha\n",
    "            dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "            preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26fbc5ea-eb3c-4b2a-a490-9d03affb3e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.7046, Accuracy = 0.7125\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.7038, Accuracy = 0.7111\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.7254, Accuracy = 0.7304\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.7237, Accuracy = 0.7286\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.7259, Accuracy = 0.7310\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_decs_full(new_matches,new_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "620ac08a-09e7-474f-9d1b-3e3131a250aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.6225, Accuracy = 0.6361\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.6254, Accuracy = 0.6425\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.5902, Accuracy = 0.6089\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6508, Accuracy = 0.6606\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6067, Accuracy = 0.6227\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_decs_full(new_matches,new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7519af8-04db-42a9-b2c6-340c7edaac6e",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging is an ensemble learning technique where multiple models (typically decision trees) are trained independently on different random subsets of the data, and their predictions are aggregated to make a final decision. \n",
    "\n",
    "## How It Works:\n",
    "- **Bootstrap Sampling**: Random subsets of data are sampled with replacement.\n",
    "- **Parallel Training**: Each model is trained independently on its own subset of data.\n",
    "- **Aggregation**: The results from all models are combined, often by majority voting (classification) or averaging (regression).\n",
    "- **Reduces Variance**: Bagging helps make the model more stable and less sensitive to fluctuations in the data.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Reduces Overfitting**: Combining multiple models reduces the likelihood of overfitting.  \n",
    "✅ **Improves Accuracy**: Bagging tends to improve accuracy compared to individual models.  \n",
    "✅ **Stable Predictions**: By aggregating predictions, the model becomes more robust.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Increased Computational Cost**: Bagging requires training multiple models, which can be computationally expensive.  \n",
    "❌ **Less Interpretability**: As it uses an ensemble of models, bagging may be less interpretable compared to single decision trees.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1fad91d1-d8fd-457b-9449-df93b3c78788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Bagging\n",
    "def make_yearly_predictions_bagging(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "            # Train a Bagging Classifier with multiple Decision Trees\n",
    "            base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "            bagging_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "            preds = bagging_clf.predict(test_year[static_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1ad96b38-ed39-4c13-9c76-36eecd8cc022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2013 with 380 matches.\n",
      "Year 2013: Precision = 0.6236, Accuracy = 0.6421\n",
      "\n",
      "Testing on year: 2014 with 760 matches.\n",
      "Year 2014: Precision = 0.6698, Accuracy = 0.6776\n",
      "\n",
      "Testing on year: 2015 with 760 matches.\n",
      "Year 2015: Precision = 0.6413, Accuracy = 0.6579\n",
      "\n",
      "Testing on year: 2016 with 756 matches.\n",
      "Year 2016: Precision = 0.7064, Accuracy = 0.7103\n",
      "\n",
      "Testing on year: 2017 with 802 matches.\n",
      "Year 2017: Precision = 0.6817, Accuracy = 0.6895\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.6485, Accuracy = 0.6637\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3cda0241-5208-4d7c-833e-e1f3bdd2e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 398 matches.\n",
      "Year 2019: Precision = 0.5875, Accuracy = 0.6055\n",
      "\n",
      "Testing on year: 2020 with 672 matches.\n",
      "Year 2020: Precision = 0.5968, Accuracy = 0.6116\n",
      "\n",
      "Testing on year: 2021 with 816 matches.\n",
      "Year 2021: Precision = 0.5842, Accuracy = 0.6005\n",
      "\n",
      "Testing on year: 2022 with 722 matches.\n",
      "Year 2022: Precision = 0.6036, Accuracy = 0.6163\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6048, Accuracy = 0.6181\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4d8e42ae-8091-4ab0-9c42-808062301ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "           # Train a Bagging Classifier with multiple Decision Trees\n",
    "            base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "            bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "768c0295-23ef-401c-8ccd-954730cd5e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.7918, Accuracy = 0.7844\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.8170, Accuracy = 0.8096\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.8303, Accuracy = 0.8207\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.8496, Accuracy = 0.8405\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.8290, Accuracy = 0.8099\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging_rolling(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ad581bf7-610d-4937-a3aa-39385a686367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.6437, Accuracy = 0.6568\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.6119, Accuracy = 0.6335\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.6007, Accuracy = 0.6224\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6345, Accuracy = 0.6467\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6145, Accuracy = 0.6343\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging_rolling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5e452fe2-52ce-467c-b6bd-9d8c590bcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "           # Train a Bagging Classifier with multiple Decision Trees\n",
    "            base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "            bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a433cf2f-19a6-4d5b-9876-ed0b3bfa0790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.8054, Accuracy = 0.8063\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.7896, Accuracy = 0.7883\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.8138, Accuracy = 0.8127\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.8294, Accuracy = 0.8279\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.8034, Accuracy = 0.7982\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging_full(new_matches,new_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "52e478e7-686b-4584-8c45-9cf84ae56a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.6148, Accuracy = 0.6272\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.6380, Accuracy = 0.6516\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.6333, Accuracy = 0.6433\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6657, Accuracy = 0.6732\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6033, Accuracy = 0.6204\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_bagging_full(new_matches,new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7f57b-aa7d-4978-a7db-dd49fbf7dbc3",
   "metadata": {},
   "source": [
    "# Random Forest  \n",
    "\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. It is an extension of Bagging that introduces additional randomness by selecting a subset of features for each tree.  \n",
    "\n",
    "## How It Works:  \n",
    "- **Bootstrap Sampling**: Each tree is trained on a different random subset of the training data (with replacement).  \n",
    "- **Feature Randomness**: Instead of considering all features at each split, only a random subset is used, making trees more diverse.  \n",
    "- **Parallel Training**: Trees are trained independently, allowing efficient computation.  \n",
    "- **Aggregation**: Predictions from all trees are combined using majority voting (for classification) or averaging (for regression).  \n",
    "\n",
    "## Advantages:  \n",
    "✅ **Reduces Overfitting**: Random selection of data and features prevents individual trees from overfitting.  \n",
    "✅ **Improves Accuracy**: Typically achieves higher accuracy than individual decision trees.  \n",
    "✅ **Handles High-Dimensional Data**: Works well with many features and avoids over-relying on any one feature.  \n",
    "✅ **Works Well with Missing Data**: Can handle missing values better than a single decision tree.  \n",
    "\n",
    "## Disadvantages:  \n",
    "❌ **Increased Computational Cost**: Training multiple trees requires more computation and memory.  \n",
    "❌ **Less Interpretability**: A single decision tree is easier to interpret than a forest of trees.  \n",
    "❌ **Can Be Slow for Real-Time Predictions**: Large forests may slow down inference for large datasets.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ab61482c-1a81-46d5-a64c-3bf779ecc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Random Forest\n",
    "def make_yearly_predictions_rf(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "   \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "            # Train a Random Forest Classifier\n",
    "            rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1, n_jobs=-1)\n",
    "            rf_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "            # Access each tree and apply pruning\n",
    "            for tree in rf_clf.estimators_:\n",
    "                tree.set_params(ccp_alpha=best_alpha) \n",
    "            # After pruning, you can predict\n",
    "            preds = rf_clf.predict(test_year[static_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ef6ace29-53cc-493c-92cb-086ba58be574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2013 with 380 matches.\n",
      "Year 2013: Precision = 0.7362, Accuracy = 0.7342\n",
      "\n",
      "Testing on year: 2014 with 760 matches.\n",
      "Year 2014: Precision = 0.6986, Accuracy = 0.7026\n",
      "\n",
      "Testing on year: 2015 with 760 matches.\n",
      "Year 2015: Precision = 0.6853, Accuracy = 0.6934\n",
      "\n",
      "Testing on year: 2016 with 756 matches.\n",
      "Year 2016: Precision = 0.7168, Accuracy = 0.7209\n",
      "\n",
      "Testing on year: 2017 with 802 matches.\n",
      "Year 2017: Precision = 0.7286, Accuracy = 0.7257\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.7003, Accuracy = 0.6988\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_rf(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9263483e-d4d1-48ea-92f9-1ac719bf104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 398 matches.\n",
      "Year 2019: Precision = 0.5730, Accuracy = 0.5980\n",
      "\n",
      "Testing on year: 2020 with 672 matches.\n",
      "Year 2020: Precision = 0.5864, Accuracy = 0.6071\n",
      "\n",
      "Testing on year: 2021 with 816 matches.\n",
      "Year 2021: Precision = 0.5914, Accuracy = 0.6127\n",
      "\n",
      "Testing on year: 2022 with 722 matches.\n",
      "Year 2022: Precision = 0.5882, Accuracy = 0.6066\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.5763, Accuracy = 0.5995\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_rf(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "307c0eb8-4a4c-46d7-9237-7816074f5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_rf_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "          # Train a Random Forest Classifier\n",
    "            rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1, n_jobs=-1)\n",
    "            rf_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "            preds = rf_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e2c273f-6563-426e-8aaa-cae63a9d618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.8267, Accuracy = 0.8094\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.8304, Accuracy = 0.8176\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.8432, Accuracy = 0.8274\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.8526, Accuracy = 0.8379\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.8262, Accuracy = 0.7982\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_rf_rolling(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e0995072-e00e-491c-8e43-b3e18087d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.6591, Accuracy = 0.6686\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.5996, Accuracy = 0.6275\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.6063, Accuracy = 0.6285\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6404, Accuracy = 0.6495\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6333, Accuracy = 0.6481\n"
     ]
    }
   ],
   "source": [
    "#TestingPrecision and Accuracy\n",
    "results = make_yearly_predictions_rf_rolling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9ca536a9-48cb-4bea-99cd-94314679096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_rf_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\",\"Rank\",\"IsRanked\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "          # Train a Random Forest Classifier\n",
    "            rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1, n_jobs=-1)\n",
    "            rf_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "            preds = rf_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "08e2ab4e-e8fe-48f5-bedb-474837732c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.8453, Accuracy = 0.8219\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.8471, Accuracy = 0.8322\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.8505, Accuracy = 0.8353\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.8673, Accuracy = 0.8492\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.8497, Accuracy = 0.8216\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results = make_yearly_predictions_rf_full(new_matches,new_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8230750b-af9e-4fa6-bb9e-42cef38231ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.6460, Accuracy = 0.6598\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.6115, Accuracy = 0.6350\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.6340, Accuracy = 0.6470\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6522, Accuracy = 0.6592\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6426, Accuracy = 0.6528\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results = make_yearly_predictions_rf_full(new_matches,new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620c1f2-19a9-45fa-ac6e-e2b694ba297f",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner. Unlike Bagging and Random Forest, which build trees independently, Boosting trains trees sequentially, where each new tree corrects the errors of the previous ones.\n",
    "\n",
    "#### How It Works:\n",
    "- **Sequential Training**: Trees are trained one after another, with each tree focusing on the misclassified instances of the previous tree.\n",
    "- **Weighted Data**: In each iteration, the incorrectly classified instances are given more weight, so subsequent trees focus more on difficult cases.\n",
    "- **Aggregation**: The final prediction is made by combining the weighted predictions of all trees, typically using a weighted vote (for classification) or weighted average (for regression).\n",
    "\n",
    "#### Advantages:\n",
    "✅ **Reduces Bias**: Boosting improves the accuracy by reducing both bias and variance.  \n",
    "✅ **Highly Accurate**: Often produces better results than individual models due to its focus on correcting errors.  \n",
    "✅ **Works Well with Complex Data**: Can handle complex data distributions and capture subtle patterns in the data.  \n",
    "✅ **Flexible**: Can be applied to a wide range of models, and different base learners (like decision trees, logistic regression, etc.) can be used.\n",
    "\n",
    "#### Disadvantages:\n",
    "❌ **Prone to Overfitting**: If too many trees are added, boosting can overfit the training data, especially if the base learner is too complex.  \n",
    "❌ **Computationally Expensive**: Training sequential trees can be slow and resource-intensive.  \n",
    "❌ **Less Interpretability**: Like Random Forest, boosting ensembles (e.g., Gradient Boosting) can be difficult to interpret.  \n",
    "❌ **Sensitive to Noisy Data**: Boosting can be sensitive to noise in the data, as it places more weight on difficult cases, which might be noisy or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b5cb3e0d-acdb-48a7-8918-6d70d752f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Gradient Boosting\n",
    "def make_yearly_predictions_gb(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "   # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "\n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "   \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "\n",
    "            # Train a Gradient Boosting Classifier\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "            gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "            # After training, you can predict\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c88c1c31-a4cf-40e1-b5d6-c55888c07fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing on year: 2013 with 380 matches.\n",
      "Year 2013: Precision = 0.7740, Accuracy = 0.7763\n",
      "\n",
      "Testing on year: 2014 with 760 matches.\n",
      "Year 2014: Precision = 0.7246, Accuracy = 0.7289\n",
      "\n",
      "Testing on year: 2015 with 760 matches.\n",
      "Year 2015: Precision = 0.7076, Accuracy = 0.7145\n",
      "\n",
      "Testing on year: 2016 with 756 matches.\n",
      "Year 2016: Precision = 0.7472, Accuracy = 0.7500\n",
      "\n",
      "Testing on year: 2017 with 802 matches.\n",
      "Year 2017: Precision = 0.7466, Accuracy = 0.7469\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.7259, Accuracy = 0.7281\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8abab01a-08b3-464d-9607-43d4bf3b5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 398 matches.\n",
      "Year 2019: Precision = 0.5379, Accuracy = 0.6055\n",
      "\n",
      "Testing on year: 2020 with 672 matches.\n",
      "Year 2020: Precision = 0.5969, Accuracy = 0.6280\n",
      "\n",
      "Testing on year: 2021 with 816 matches.\n",
      "Year 2021: Precision = 0.6706, Accuracy = 0.6373\n",
      "\n",
      "Testing on year: 2022 with 722 matches.\n",
      "Year 2022: Precision = 0.6500, Accuracy = 0.6247\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.6244, Accuracy = 0.6273\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca1eaf64-f4e7-4d67-ad97-7bedc8ffc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\",]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "         # Train a Gradient Boosting Classifier\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1)\n",
    "            gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "            # After training, you can predict\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "  \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f8ef50d6-f579-449a-bd88-0f0530699c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.7382, Accuracy = 0.7438\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.7141, Accuracy = 0.7204\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.7516, Accuracy = 0.7543\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.7458, Accuracy = 0.7462\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.7399, Accuracy = 0.7398\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb_rolling(matches,matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a9d2e9ea-380d-47b2-9c9d-171ff934b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.5486, Accuracy = 0.5710\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.5762, Accuracy = 0.5807\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.5811, Accuracy = 0.5916\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.5855, Accuracy = 0.5939\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.5848, Accuracy = 0.5949\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb_rolling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "45a1a276-c833-4b3e-b65a-5b1c60414e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\",]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    yearly_results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            print(f\"\\nTesting on year: {year} with {len(test_year)} matches.\")\n",
    "            \n",
    "         # Train a Gradient Boosting Classifier\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1)\n",
    "            gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "            # After training, you can predict\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "  \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            print(f\"Year {year}: Precision = {precision:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "93b42780-952c-4b6b-9c85-b416509cc755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2014 with 320 matches.\n",
      "Year 2014: Precision = 0.7482, Accuracy = 0.7531\n",
      "\n",
      "Testing on year: 2015 with 751 matches.\n",
      "Year 2015: Precision = 0.7126, Accuracy = 0.7190\n",
      "\n",
      "Testing on year: 2016 with 753 matches.\n",
      "Year 2016: Precision = 0.7492, Accuracy = 0.7530\n",
      "\n",
      "Testing on year: 2017 with 796 matches.\n",
      "Year 2017: Precision = 0.7462, Accuracy = 0.7487\n",
      "\n",
      "Testing on year: 2018 with 342 matches.\n",
      "Year 2018: Precision = 0.7320, Accuracy = 0.7339\n"
     ]
    }
   ],
   "source": [
    "#Training Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb_full(new_matches,new_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c1390144-b421-47e7-af4e-d0ab53380b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.000865 with Accuracy: 0.6453\n",
      "\n",
      "Testing on year: 2019 with 338 matches.\n",
      "Year 2019: Precision = 0.5764, Accuracy = 0.6006\n",
      "\n",
      "Testing on year: 2020 with 663 matches.\n",
      "Year 2020: Precision = 0.5854, Accuracy = 0.5867\n",
      "\n",
      "Testing on year: 2021 with 813 matches.\n",
      "Year 2021: Precision = 0.5575, Accuracy = 0.5633\n",
      "\n",
      "Testing on year: 2022 with 719 matches.\n",
      "Year 2022: Precision = 0.6209, Accuracy = 0.6300\n",
      "\n",
      "Testing on year: 2023 with 432 matches.\n",
      "Year 2023: Precision = 0.5987, Accuracy = 0.6157\n"
     ]
    }
   ],
   "source": [
    "#Testing Precision and Accuracy\n",
    "results =  make_yearly_predictions_gb_full(new_matches,new_test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b9ee7-1018-43f5-809f-2bb0b0ade93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
