{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700635c0-6908-45c0-bdb0-b61813abaa74",
   "metadata": {},
   "source": [
    "# Decision Trees and Their Application in Classification  \n",
    "\n",
    "In this section, we will **test** and **evaluate** various tree-based methods, including **classification trees**, **bagging**, **random forest**, and **boosting**, to see which works best for predicting football match outcomes.\n",
    "\n",
    "## What Are Decision Trees?  \n",
    "\n",
    "A **decision tree** is a supervised machine learning algorithm used for both **classification** and **regression** tasks. It splits the data into subsets based on the most significant feature at each step, resulting in a tree-like structure. \n",
    "\n",
    "### How Do Decision Trees Work for Classification?  \n",
    "\n",
    "In **classification tasks**, decision trees are used to predict categorical outcomes by recursively splitting the data at each node, with the goal of maximizing the \"purity\" of the resulting subsets. The **root node** represents the entire dataset, and the tree branches out to **leaf nodes** that represent the predicted class label. Each split is based on the feature that best separates the data at that point, typically using a metric such as **Gini impurity** or **cross entropy**.\n",
    "\n",
    "#### Example Process:\n",
    "1. **Starting Node (Root)**: The algorithm evaluates all possible features and chooses the one that best divides the data into distinct classes.\n",
    "2. **Internal Nodes**: Each subsequent node splits the data based on a feature that provides the greatest separation of class labels.\n",
    "3. **Leaf Nodes**: These represent the final predicted class labels for a given subset of data.\n",
    "\n",
    "Decision trees are **easy to interpret** and visualize, which makes them an appealing choice for understanding how predictions are made. However, they can be prone to **overfitting** if not properly tuned.\n",
    "\n",
    "## Types of Tree-Based Methods  \n",
    "\n",
    "#### 1. Classification Trees  \n",
    "#### 2. Bagging (Bootstrap Aggregating)  \n",
    "#### 3. Random Forest  \n",
    "#### 4. Boosting  \n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this section, we have explored various tree-based methodsâ€”**classification trees**, **bagging**, **random forest**, and **boosting**â€”and will test each one to determine which best suits the football match outcome prediction task. Each method will be tested and evaluated to assess its effectiveness in predicting match results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37915954-7569-417e-9127-ce9bdca85716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0655eca3-7e2a-4f55-919b-8bbe7fde4fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea9c0d0-e919-411f-88fb-254cb8c51d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970b8815-d6e5-4de9-9196-ed978106523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Ultimate_Hyperparameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d263c3-b639-4b88-9346-990a3a41b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0348ebb9-13cd-4ae7-8de1-13559cd85e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19d4210-4aca-4714-908d-4595702758fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2128ddab-b709-47a4-ae9a-0850e13241f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef9138b-0535-4aa8-83d2-822e0d6f3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b967b7-49c7-4b64-82df-3bc53cbacda4",
   "metadata": {},
   "source": [
    "# Best Model Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812d0bd-dbd7-4820-a9b7-7eab8ef11c99",
   "metadata": {},
   "source": [
    "## Choosing the Best Overall Model\n",
    "\n",
    "In our analysis, we have evaluated four different machine learning models: **Bagging, Gradient Boosting, Decision Tree, and Random Forest**. Each of these models has been tested on multiple years of data, and their performance has been measured using **accuracy** and **precision**.\n",
    "\n",
    "### Why Do We Need to Choose the Best Model?\n",
    "\n",
    "1. **Consistency Across Years**  \n",
    "   Some models may perform well in certain years but not in others. Selecting the best overall model ensures that we choose the one that provides **consistent performance across multiple years** rather than excelling only in specific cases.\n",
    "\n",
    "2. **Generalization to Future Data**  \n",
    "   The goal is to make predictions on new, unseen data. A model that performs well across different years is more **robust and reliable** for future predictions.\n",
    "\n",
    "3. **Maximizing Accuracy and Precision**  \n",
    "   By comparing the performance of all models, we can **identify the model with the highest accuracy and precision**. This helps in minimizing errors and improving decision-making.\n",
    "\n",
    "4. **Avoiding Overfitting**  \n",
    "   Some models might perform exceptionally well on training data but fail on test data. By analyzing results across multiple years, we ensure that the chosen model is **not overfitting** and generalizes well.\n",
    "\n",
    "### How Will We Choose the Best Model?\n",
    "We will compare all models based on:\n",
    "- **Accuracy** (how often the model makes correct predictions)  \n",
    "- **Precision** (how reliable the model's positive predictions are)  \n",
    "\n",
    "The model that consistently achieves the highest accuracy and precision across multiple years will be selected as the **best overall model**.\n",
    "\n",
    "By following this approach, we ensure that our chosen model is the most reliable and effective for making predictions. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194982f-4143-437e-bc68-d727346c52dc",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c732be7e-ca2c-4395-8c01-6a8b5971411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_tree_model_baseline(A, B):\n",
    "    bagging_results = make_yearly_predictions_bagging(A, B)\n",
    "    gb_results = make_yearly_predictions_gb(A, B)\n",
    "    decs_results = make_yearly_predictions_decs(A, B)\n",
    "    rf_results = make_yearly_predictions_rf(A, B)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = pd.concat([bagging_results, gb_results, decs_results, rf_results], ignore_index=True)\n",
    "\n",
    "    # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1659e-c352-4f5f-a209-fa2acadc2749",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f03d9f-d488-43b0-9970-42fe21338805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_tree_model_rollling(A,B) :\n",
    "    bagging_results = make_yearly_predictions_bagging_rolling(A,B)\n",
    "    gb_results = make_yearly_predictions_gb_rolling(A,B)\n",
    "    decs_results = make_yearly_predictions_decs_rolling(A,B)\n",
    "    rf_results = make_yearly_predictions_rf_rolling(A,B)\n",
    "\n",
    "   # Combine all results\n",
    "    all_results = pd.concat([bagging_results, gb_results, decs_results, rf_results], ignore_index=True)\n",
    "\n",
    "   # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "\n",
    "  # Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f3e95-a7d0-4567-9e94-94e2dec07da5",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0314b749-0339-45e8-9cd5-790a86fa4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_tree_model_full(A,B) :\n",
    "    bagging_results = make_yearly_predictions_bagging_full(A,B)\n",
    "    gb_results = make_yearly_predictions_gb_full(A,B)\n",
    "    decs_results = make_yearly_predictions_decs_full(A,B)\n",
    "    rf_results = make_yearly_predictions_rf_full(A,B)\n",
    "\n",
    "   # Combine all results\n",
    "    all_results = pd.concat([bagging_results, gb_results, decs_results, rf_results], ignore_index=True)\n",
    "\n",
    "   # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "# Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a6d6b-ca56-4ac8-9667-c68efd36bf40",
   "metadata": {},
   "source": [
    "## Importance of Training Accuracy \n",
    "\n",
    "Training accuracy measures how well a machine learning model fits the training data. It is important to check training accuracy for the following reasons:\n",
    "\n",
    "1. **Detecting Underfitting**  \n",
    "   - If the training accuracy is **too low**, it means the model is **not learning enough** patterns from the data.  \n",
    "   - This could be due to an **overly simple model**, insufficient features, or poor hyperparameters.\n",
    "\n",
    "2. **Ensuring Model Competency**  \n",
    "   - A model with **reasonable training accuracy** ensures that it has successfully learned meaningful patterns from the dataset.  \n",
    "   - If the model cannot achieve high accuracy on the training data, it is unlikely to perform well on new data.\n",
    "\n",
    "3. **Providing a Baseline for Comparison**  \n",
    "   - Training accuracy helps us **compare** with testing accuracy to detect **overfitting**.  \n",
    "   - If training accuracy is significantly higher than testing accuracy, the model might be **memorizing** rather than **generalizing**.\n",
    "\n",
    "ðŸ’¡ **Key Insight**: While high training accuracy is desirable, it should not be the sole indicator of a good model. We must also check testing accuracy to ensure real-world performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60522783-85a4-415a-b938-2b3bc6878e20",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1639db6-6f49-4795-b3b1-5cfdfd0c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_baseline(matches,matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2e547-f338-443f-9cdb-c5026f7b63d6",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "629387fa-2987-47bd-a112-eb1f7c1dc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_rollling(matches,matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f517650-f162-4e72-bcb1-d3bcf67e4b60",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f7317b-5364-4c40-8c99-5f285868998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_tree_model_full(new_matches,new_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7649a-8ef2-427c-bf92-793d952a5e65",
   "metadata": {},
   "source": [
    "## Importance of Checking Testing Accuracy\n",
    "\n",
    "Testing accuracy measures how well a machine learning model performs on **unseen** data. It is crucial to check testing accuracy for the following reasons:\n",
    "\n",
    "1. **Evaluating Generalization**  \n",
    "   - The primary goal of machine learning is to create models that **generalize well** to new data.  \n",
    "   - A high testing accuracy indicates that the model can make reliable predictions on unseen samples.\n",
    "\n",
    "2. **Detecting Overfitting**  \n",
    "   - If the training accuracy is high but the testing accuracy is low, it suggests **overfitting**.  \n",
    "   - Overfitting occurs when the model learns **specific details** of the training data rather than general patterns, making it unreliable for new data.\n",
    "\n",
    "3. **Validating Model Performance**  \n",
    "   - A model is only useful if it performs well on real-world data.  \n",
    "   - Testing accuracy gives us a **realistic expectation** of how the model will behave when deployed.\n",
    "\n",
    "4. **Comparing Different Models**  \n",
    "   - By evaluating testing accuracy across different models, we can select the best model for **real-world applications**.  \n",
    "   - The model with the highest **testing accuracy and precision** is often the best choice.\n",
    "\n",
    "ðŸ’¡ **Key Insight**: A good model should have **both high training and testing accuracy**. A balance between these ensures that the model is neither too simple (underfitting) nor too complex (overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990125d7-78f1-4f91-8757-3e5d32f332a2",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3b18b9-41ff-4700-b2d4-4faa40f1db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_baseline(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20723dec-fcf3-49f4-be25-4bffe47aebbf",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "666a99f4-232b-4c81-b8e6-434e6c74b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_rollling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e4ef8-8ee1-4a1e-acd3-50e2aadd5bd3",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68eb9c35-a276-4564-aff3-2e287a5bc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_full(new_matches,new_test_matches)#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
