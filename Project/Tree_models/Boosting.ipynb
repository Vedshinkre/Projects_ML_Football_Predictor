{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b0dd3f-fda8-4f3f-bd09-762e5f09b63d",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner. Unlike Bagging and Random Forest, which build trees independently, Boosting trains trees sequentially, where each new tree corrects the errors of the previous ones.\n",
    "\n",
    "#### How It Works:\n",
    "- **Sequential Training**: Trees are trained one after another, with each tree focusing on the misclassified instances of the previous tree.\n",
    "- **Weighted Data**: In each iteration, the incorrectly classified instances are given more weight, so subsequent trees focus more on difficult cases.\n",
    "- **Aggregation**: The final prediction is made by combining the weighted predictions of all trees, typically using a weighted vote (for classification) or weighted average (for regression).\n",
    "\n",
    "#### Advantages:\n",
    "✅ **Reduces Bias**: Boosting improves the accuracy by reducing both bias and variance.  \n",
    "✅ **Highly Accurate**: Often produces better results than individual models due to its focus on correcting errors.  \n",
    "✅ **Works Well with Complex Data**: Can handle complex data distributions and capture subtle patterns in the data.  \n",
    "✅ **Flexible**: Can be applied to a wide range of models, and different base learners (like decision trees, logistic regression, etc.) can be used.\n",
    "\n",
    "#### Disadvantages:\n",
    "❌ **Prone to Overfitting**: If too many trees are added, boosting can overfit the training data, especially if the base learner is too complex.  \n",
    "❌ **Computationally Expensive**: Training sequential trees can be slow and resource-intensive.  \n",
    "❌ **Less Interpretability**: Like Random Forest, boosting ensembles (e.g., Gradient Boosting) can be difficult to interpret.  \n",
    "❌ **Sensitive to Noisy Data**: Boosting can be sensitive to noise in the data, as it places more weight on difficult cases, which might be noisy or outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec83f8-3eeb-4d8d-a11c-e53cfd8a40e7",
   "metadata": {},
   "source": [
    "### Boosting using Baseline Predictors (refer /Data/Data_Formatting.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ae93c4-0d7e-4e7a-a196-6cad68529d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Gradient Boosting\n",
    "def make_yearly_predictions_gb(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_base(Train)\n",
    "   \n",
    "    # Define static predictors\n",
    "    static_predictors =  parameters_base(Train,Test)\n",
    "    \n",
    "      # Train a Gradient Boosting Classifier\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "           # Predict on test data\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "            \n",
    "             \n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a5648-46e5-4c7e-b28b-32cce5016b86",
   "metadata": {},
   "source": [
    "### Boosting using Baseline Predictors + Rolling Predictors (refer /Data/Data_Formatting.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da18489c-486a-4a6d-a21e-f69b6c932422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_rolling(Train, Test):\n",
    "    results = []\n",
    "    all_predictors = parameters_roll(Train, Test)\n",
    "    \n",
    "    # Process all data together for proper rolling calculations\n",
    "    full_data = pd.concat([Train, Test]).sort_values(['Team', 'Date'])\n",
    "    full_data = roll(full_data)\n",
    "    \n",
    "    # Split back into train/test\n",
    "    Train = full_data[full_data['Date'].isin(Train['Date'])]\n",
    "    Test = full_data[full_data['Date'].isin(Test['Date'])]\n",
    "    \n",
    "    for year in sorted(Test['Date'].dt.year.unique()):\n",
    "        # Train on all data BEFORE the test year\n",
    "        train_mask = Train['Date'].dt.year < year\n",
    "        X_train = Train.loc[train_mask, all_predictors]\n",
    "        y_train = Train.loc[train_mask, \"Target\"]\n",
    "        \n",
    "        # Test on current year\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        X_test = test_year[all_predictors]\n",
    "        y_test = test_year[\"Target\"]\n",
    "        \n",
    "        if len(X_train) > 0 and len(X_test) > 0:\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, \n",
    "                                             min_samples_split=10, random_state=1)\n",
    "            gb_clf.fit(X_train, y_train)\n",
    "            \n",
    "            preds = gb_clf.predict(X_test)\n",
    "            precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Samples\": len(y_test)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d618ffc-8401-4f00-a607-545e6d2e3148",
   "metadata": {},
   "source": [
    "### Boosting using Full Feature Set (refer /Data/Data_Formatting.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd81676f-cd14-4d5a-a332-da9ad4b83b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_full(Train, Test):\n",
    "    results = []\n",
    "    all_predictors = parameters_full(Train, Test)\n",
    "    \n",
    "    # Process all data together for proper rolling calculations\n",
    "    full_data = pd.concat([Train, Test]).sort_values(['Team', 'Date'])\n",
    "    full_data = roll(full_data)\n",
    "    \n",
    "    # Split back into train/test\n",
    "    Train = full_data[full_data['Date'].isin(Train['Date'])]\n",
    "    Test = full_data[full_data['Date'].isin(Test['Date'])]\n",
    "    \n",
    "    for year in sorted(Test['Date'].dt.year.unique()):\n",
    "        # Train on all data BEFORE the test year\n",
    "        train_mask = Train['Date'].dt.year < year\n",
    "        X_train = Train.loc[train_mask, all_predictors]\n",
    "        y_train = Train.loc[train_mask, \"Target\"]\n",
    "        \n",
    "        # Test on current year\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        X_test = test_year[all_predictors]\n",
    "        y_test = test_year[\"Target\"]\n",
    "        \n",
    "        if len(X_train) > 0 and len(X_test) > 0:\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, \n",
    "                                             min_samples_split=10, random_state=1)\n",
    "            gb_clf.fit(X_train, y_train)\n",
    "            \n",
    "            preds = gb_clf.predict(X_test)\n",
    "            precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            \n",
    "            results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Samples\": len(y_test)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
