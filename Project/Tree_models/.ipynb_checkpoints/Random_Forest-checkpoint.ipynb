{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fdac94-e476-41d3-a1ae-6df0d1865df8",
   "metadata": {},
   "source": [
    "# Random Forest  \n",
    "\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. It is an extension of Bagging that introduces additional randomness by selecting a subset of features for each tree.  \n",
    "\n",
    "## How It Works:  \n",
    "- **Bootstrap Sampling**: Each tree is trained on a different random subset of the training data (with replacement).  \n",
    "- **Feature Randomness**: Instead of considering all features at each split, only a random subset is used, making trees more diverse.  \n",
    "- **Parallel Training**: Trees are trained independently, allowing efficient computation.  \n",
    "- **Aggregation**: Predictions from all trees are combined using majority voting (for classification) or averaging (for regression).  \n",
    "\n",
    "## Advantages:  \n",
    "✅ **Reduces Overfitting**: Random selection of data and features prevents individual trees from overfitting.  \n",
    "✅ **Improves Accuracy**: Typically achieves higher accuracy than individual decision trees.  \n",
    "✅ **Handles High-Dimensional Data**: Works well with many features and avoids over-relying on any one feature.  \n",
    "✅ **Works Well with Missing Data**: Can handle missing values better than a single decision tree.  \n",
    "\n",
    "## Disadvantages:  \n",
    "❌ **Increased Computational Cost**: Training multiple trees requires more computation and memory.  \n",
    "❌ **Less Interpretability**: A single decision tree is easier to interpret than a forest of trees.  \n",
    "❌ **Can Be Slow for Real-Time Predictions**: Large forests may slow down inference for large datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d76a1-16d5-4711-8af1-b9975a1bb234",
   "metadata": {},
   "source": [
    "### Random Forest using Baseline Predictors     (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec955161-85aa-420a-9dc5-fd47c2c44706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Random Forest\n",
    "def make_yearly_predictions_rf(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_base(Train)\n",
    "\n",
    "    # Define static predictors\n",
    "    static_predictors =  parameters_base(Train,Test)\n",
    "    \n",
    "    # Train a Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1, n_jobs=-1)\n",
    "    rf_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "     # Access each tree and apply pruning\n",
    "    for tree in rf_clf.estimators_:\n",
    "        tree.set_params(ccp_alpha=best_alpha) \n",
    "                \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:  \n",
    "            # After pruning, you can predict\n",
    "            preds = rf_clf.predict(test_year[static_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "                # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Random Forest\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27e008-8a32-41d2-9631-31c718805c6d",
   "metadata": {},
   "source": [
    "### Random Forest using Baseline Predictors + Rolling Predictors (refer /Data/Data_Formatting.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5123e6-eb6e-4c10-a10c-2d6a5f242ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_rf_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_roll(Train)\n",
    "\n",
    "    all_predictors = parameters_roll(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train a Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1, n_jobs=-1)\n",
    "    rf_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "           preds = rf_clf.predict(test_year[ all_predictors])\n",
    "           # Calculate precision and accuracy \n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Random Forest\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae18cd-b949-43b8-9a5e-eb14644c2386",
   "metadata": {},
   "source": [
    "### Random Forest using Full Feature Set  (refer /Data/Data_Formatting.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4416aba3-7063-43d7-9e84-17591456b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_rf_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_full(Train)\n",
    "\n",
    "    all_predictors = parameters_full(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "    \n",
    "     # Train a Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1, n_jobs=-1)\n",
    "    rf_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = rf_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "         # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Random Forest\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
