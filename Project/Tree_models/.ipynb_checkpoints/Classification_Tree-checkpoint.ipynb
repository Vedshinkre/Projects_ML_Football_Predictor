{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52995372-8e13-4670-aa1d-071cfcd1eb1e",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "\n",
    "A **Classification Tree** is a type of decision tree used for classification tasks. It works by recursively splitting the dataset into subsets based on feature values, aiming to maximize the separation between different classes. The final result is a tree-like model where each leaf node represents a class label.\n",
    "\n",
    "## How It Works:\n",
    "- **Recursive Partitioning**: The dataset is split into smaller groups using the most informative features.\n",
    "- **Gini Impurity / Entropy**: The quality of splits is determined using metrics like Gini Impurity or Entropy.\n",
    "- **Tree Growth**: The process continues until a stopping criterion is met (e.g., maximum depth, minimum samples per split).\n",
    "- **Prediction**: For a new input, the model follows the decision path and assigns a class label based on the majority vote in the final node.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Easy to Interpret**: The decision-making process is visual and intuitive.  \n",
    "✅ **Requires Minimal Data Preprocessing**: No need for feature scaling or normalization.  \n",
    "✅ **Captures Non-Linear Relationships**: Works well with complex decision boundaries.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Prone to Overfitting**: Without pruning, the tree can become too complex and fit noise in the data.  \n",
    "❌ **Unstable**: Small changes in data can result in a significantly different tree.  \n",
    "❌ **Less Accurate Than Ensembles**: Single decision trees are often outperformed by ensemble methods like Random Forests and Gradient Boosting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5841372-4a4e-4cf8-a222-f638fefa8ed9",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7cefd0a-68ed-4ab2-88f0-f9bdeb2a6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions\n",
    "def make_yearly_predictions_decs(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_base(Train)\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors =  parameters_base(Train,Test)\n",
    "   \n",
    "     # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[static_predictors], Train[\"Target\"])\n",
    "  \n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "           preds = dt.predict(test_year[static_predictors])\n",
    "           \n",
    "             # Calculate precision and accuracy\n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "           # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89b71f-69f6-452c-8593-18435bd6e6ff",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors + Rolling Predictors (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a44b77-a186-4593-a4b9-28d306cddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_roll(Train)\n",
    "\n",
    "    all_predictors = parameters_roll(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "           preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "             # Calculate precision and accuracy\n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "              # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4cec5-bacb-4484-b6b2-099df8697bf0",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Full Feature Set (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3275c3c1-dc5f-4ffe-a0cc-ef741851daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_full(Train)\n",
    "    \n",
    "    all_predictors = parameters_full(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "     \n",
    "     # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return (results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
