{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f90f998-f046-4278-985e-34e016a11615",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging is an ensemble learning technique where multiple models (typically decision trees) are trained independently on different random subsets of the data, and their predictions are aggregated to make a final decision. \n",
    "\n",
    "## How It Works:\n",
    "- **Bootstrap Sampling**: Random subsets of data are sampled with replacement.\n",
    "- **Parallel Training**: Each model is trained independently on its own subset of data.\n",
    "- **Aggregation**: The results from all models are combined, often by majority voting (classification) or averaging (regression).\n",
    "- **Reduces Variance**: Bagging helps make the model more stable and less sensitive to fluctuations in the data.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Reduces Overfitting**: Combining multiple models reduces the likelihood of overfitting.  \n",
    "✅ **Improves Accuracy**: Bagging tends to improve accuracy compared to individual models.  \n",
    "✅ **Stable Predictions**: By aggregating predictions, the model becomes more robust.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Increased Computational Cost**: Bagging requires training multiple models, which can be computationally expensive.  \n",
    "❌ **Less Interpretability**: As it uses an ensemble of models, bagging may be less interpretable compared to single decision trees.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72159e72-0952-4f87-b924-df2523429d1a",
   "metadata": {},
   "source": [
    "### Bagging using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0465ebe0-2d65-49b5-a299-112946853395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_base(Train)\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors = parameters_base(Train,Test)\n",
    "\n",
    "    # Train Bagging model on training data\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1)\n",
    "    bagging_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = accuracy_score(Train['Target'], bagging_clf.predict(Train[static_predictors]))\n",
    "\n",
    "    # Create a list to store results\n",
    "    results = []\n",
    "\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = bagging_clf.predict(test_year[static_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318d8d6-90b8-4fdd-90f1-9d68b65b42a9",
   "metadata": {},
   "source": [
    "### Bagging using Baseline Predictors + Rolling Predictors   (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4eea925-0472-4554-a759-2af637fd33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_roll(Train)\n",
    "   \n",
    "    all_predictors = parameters_roll(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train a Bagging Classifier with multiple Decision Trees\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "    bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:  \n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "             # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33b033-214b-4c35-8dfa-9c483d4abc4c",
   "metadata": {},
   "source": [
    "### Bagging using  Full Feature Set (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e6e6e73-d238-46c3-b549-0dc7f786ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha_full(Train)\n",
    "    \n",
    "    all_predictors = parameters_full(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train a Bagging Classifier with multiple Decision Trees\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "    bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "           # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
