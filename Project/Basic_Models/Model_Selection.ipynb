{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700635c0-6908-45c0-bdb0-b61813abaa74",
   "metadata": {},
   "source": [
    "# Basic Classification Models: LDA, QDA, KNN, and Logistic Regression  \n",
    "\n",
    " In this section, we explore four fundamental classification models: **Linear Discriminant Analysis (LDA)**, **Quadratic Discriminant Analysis (QDA)**, **K-Nearest Neighbors (KNN)**, and **Logistic Regression**. These models serve as the foundation for many machine learning classification problems.\n",
    " \n",
    "## 1. Linear Discriminant Analysis (LDA)  \n",
    "LDA finds a linear combination of features that best separates classes, assuming normally distributed data with the same covariance.  \n",
    "\n",
    "✅ Works well for linearly separable data  \n",
    "❌ Struggles with non-linear class boundaries  \n",
    "\n",
    "## 2. Quadratic Discriminant Analysis (QDA)  \n",
    "QDA extends LDA by allowing each class to have its own covariance matrix, leading to quadratic decision boundaries.  \n",
    "\n",
    "✅ More flexible than LDA  \n",
    "❌ Requires more data, prone to overfitting  \n",
    "\n",
    "## 3. K-Nearest Neighbors (KNN)  \n",
    "KNN classifies a point based on the majority class of its **k** closest neighbors using a distance metric.  \n",
    "\n",
    "✅ No assumptions about data distribution  \n",
    "❌ Computationally expensive, sensitive to noise  \n",
    "\n",
    "## 4. Logistic Regression  \n",
    "Logistic Regression predicts class probabilities using a sigmoid function and is best for binary classification.  \n",
    "\n",
    "✅ Simple, interpretable, works well for linear problems  \n",
    "❌ Struggles with non-linearity unless features are transformed  \n",
    "\n",
    "## Summary  \n",
    "- **LDA/QDA**: Good for normally distributed data, LDA for linear, QDA for non-linear.  \n",
    "- **KNN**: Flexible but slow on large datasets.  \n",
    "- **Logistic Regression**: Best for simple binary classification with linear relationships.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37915954-7569-417e-9127-ce9bdca85716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0655eca3-7e2a-4f55-919b-8bbe7fde4fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea9c0d0-e919-411f-88fb-254cb8c51d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970b8815-d6e5-4de9-9196-ed978106523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Ultimate_Hyperparameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d263c3-b639-4b88-9346-990a3a41b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0348ebb9-13cd-4ae7-8de1-13559cd85e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19d4210-4aca-4714-908d-4595702758fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b967b7-49c7-4b64-82df-3bc53cbacda4",
   "metadata": {},
   "source": [
    "# Best Model Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812d0bd-dbd7-4820-a9b7-7eab8ef11c99",
   "metadata": {},
   "source": [
    "## Choosing the Best Overall Model\n",
    "\n",
    "In our analysis, we have evaluated four different machine learning models: **Bagging, Gradient Boosting, Decision Tree, and Random Forest**. Each of these models has been tested on multiple years of data, and their performance has been measured using **accuracy** and **precision**.\n",
    "\n",
    "### Why Do We Need to Choose the Best Model?\n",
    "\n",
    "1. **Consistency Across Years**  \n",
    "   Some models may perform well in certain years but not in others. Selecting the best overall model ensures that we choose the one that provides **consistent performance across multiple years** rather than excelling only in specific cases.\n",
    "\n",
    "2. **Generalization to Future Data**  \n",
    "   The goal is to make predictions on new, unseen data. A model that performs well across different years is more **robust and reliable** for future predictions.\n",
    "\n",
    "3. **Maximizing Accuracy and Precision**  \n",
    "   By comparing the performance of all models, we can **identify the model with the highest accuracy and precision**. This helps in minimizing errors and improving decision-making.\n",
    "\n",
    "4. **Avoiding Overfitting**  \n",
    "   Some models might perform exceptionally well on training data but fail on test data. By analyzing results across multiple years, we ensure that the chosen model is **not overfitting** and generalizes well.\n",
    "\n",
    "### How Will We Choose the Best Model?\n",
    "We will compare all models based on:\n",
    "- **Accuracy** (how often the model makes correct predictions)  \n",
    "- **Precision** (how reliable the model's positive predictions are)  \n",
    "\n",
    "The model that consistently achieves the highest accuracy and precision across multiple years will be selected as the **best overall model**.\n",
    "\n",
    "By following this approach, we ensure that our chosen model is the most reliable and effective for making predictions. 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194982f-4143-437e-bc68-d727346c52dc",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c732be7e-ca2c-4395-8c01-6a8b5971411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_basic_model_baseline(A, B):\n",
    "    KNN_results = make_yearly_predictions_knn_base(A, B)\n",
    "    LDA_results =make_yearly_predictions_lda_base(A, B)\n",
    "    LR_results = make_yearly_predictions_lr_base(A, B)\n",
    "    QDA_results = make_yearly_predictions_qda_base(A, B)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = pd.concat([KNN_results, LDA_results, LR_results, QDA_results], ignore_index=True)\n",
    "\n",
    "    # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1659e-c352-4f5f-a209-fa2acadc2749",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f03d9f-d488-43b0-9970-42fe21338805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_basic_model_rolling(A, B):\n",
    "    KNN_results = make_yearly_predictions_knn_roll(A, B)\n",
    "    LDA_results =make_yearly_predictions_lda_roll(A, B)\n",
    "    LR_results = make_yearly_predictions_lr_roll(A, B)\n",
    "    QDA_results = make_yearly_predictions_qda_roll(A, B)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = pd.concat([KNN_results, LDA_results, LR_results, QDA_results], ignore_index=True)\n",
    "\n",
    "    # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f3e95-a7d0-4567-9e94-94e2dec07da5",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0314b749-0339-45e8-9cd5-790a86fa4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "def best_basic_model_full(A, B):\n",
    "    KNN_results = make_yearly_predictions_knn_full(A, B)\n",
    "    LDA_results =make_yearly_predictions_lda_full(A, B)\n",
    "    LR_results = make_yearly_predictions_lr_full(A, B)\n",
    "    QDA_results = make_yearly_predictions_qda_full(A, B)\n",
    "\n",
    "    # Combine all results\n",
    "    all_results = pd.concat([KNN_results, LDA_results, LR_results, QDA_results], ignore_index=True)\n",
    "\n",
    "    # Find best accuracy & precision for each year\n",
    "    best_per_year = all_results.loc[all_results.groupby(\"Year\")[\"Accuracy\"].idxmax()]\n",
    "    best_per_year_precision = all_results.loc[all_results.groupby(\"Year\")[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Compute average precision and accuracy per model\n",
    "    avg_results = all_results.groupby(\"Model\")[[\"Accuracy\", \"Precision\"]].mean().reset_index()\n",
    "\n",
    "    # Find the best overall model based on highest average accuracy\n",
    "    best_model = avg_results.loc[avg_results[\"Accuracy\"].idxmax()]\n",
    "\n",
    "    # Find the best overall model based on highest average precision\n",
    "    best_precision_model = avg_results.loc[avg_results[\"Precision\"].idxmax()]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Best Model Per Year (by Accuracy):\")\n",
    "    print(best_per_year)\n",
    "\n",
    "    print(\"\\nBest Model Per Year (by Precision):\")\n",
    "    print(best_per_year_precision)\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Accuracy):\")\n",
    "    print(f\"Model: {best_model['Model']}, Avg Precision: {best_model['Precision']:.4f}, Avg Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\nOverall Best Model (by Precision):\")\n",
    "    print(f\"Model: {best_precision_model['Model']}, Avg Precision: {best_precision_model['Precision']:.4f}, Avg Accuracy: {best_precision_model['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a6d6b-ca56-4ac8-9667-c68efd36bf40",
   "metadata": {},
   "source": [
    "## Importance of Training Accuracy \n",
    "\n",
    "Training accuracy measures how well a machine learning model fits the training data. It is important to check training accuracy for the following reasons:\n",
    "\n",
    "1. **Detecting Underfitting**  \n",
    "   - If the training accuracy is **too low**, it means the model is **not learning enough** patterns from the data.  \n",
    "   - This could be due to an **overly simple model**, insufficient features, or poor hyperparameters.\n",
    "\n",
    "2. **Ensuring Model Competency**  \n",
    "   - A model with **reasonable training accuracy** ensures that it has successfully learned meaningful patterns from the dataset.  \n",
    "   - If the model cannot achieve high accuracy on the training data, it is unlikely to perform well on new data.\n",
    "\n",
    "3. **Providing a Baseline for Comparison**  \n",
    "   - Training accuracy helps us **compare** with testing accuracy to detect **overfitting**.  \n",
    "   - If training accuracy is significantly higher than testing accuracy, the model might be **memorizing** rather than **generalizing**.\n",
    "\n",
    "💡 **Key Insight**: While high training accuracy is desirable, it should not be the sole indicator of a good model. We must also check testing accuracy to ensure real-world performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60522783-85a4-415a-b938-2b3bc6878e20",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2e547-f338-443f-9cdb-c5026f7b63d6",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f517650-f162-4e72-bcb1-d3bcf67e4b60",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7649a-8ef2-427c-bf92-793d952a5e65",
   "metadata": {},
   "source": [
    "## Importance of Checking Testing Accuracy\n",
    "\n",
    "Testing accuracy measures how well a machine learning model performs on **unseen** data. It is crucial to check testing accuracy for the following reasons:\n",
    "\n",
    "1. **Evaluating Generalization**  \n",
    "   - The primary goal of machine learning is to create models that **generalize well** to new data.  \n",
    "   - A high testing accuracy indicates that the model can make reliable predictions on unseen samples.\n",
    "\n",
    "2. **Detecting Overfitting**  \n",
    "   - If the training accuracy is high but the testing accuracy is low, it suggests **overfitting**.  \n",
    "   - Overfitting occurs when the model learns **specific details** of the training data rather than general patterns, making it unreliable for new data.\n",
    "\n",
    "3. **Validating Model Performance**  \n",
    "   - A model is only useful if it performs well on real-world data.  \n",
    "   - Testing accuracy gives us a **realistic expectation** of how the model will behave when deployed.\n",
    "\n",
    "4. **Comparing Different Models**  \n",
    "   - By evaluating testing accuracy across different models, we can select the best model for **real-world applications**.  \n",
    "   - The model with the highest **testing accuracy and precision** is often the best choice.\n",
    "\n",
    "💡 **Key Insight**: A good model should have **both high training and testing accuracy**. A balance between these ensures that the model is neither too simple (underfitting) nor too complex (overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990125d7-78f1-4f91-8757-3e5d32f332a2",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3b18b9-41ff-4700-b2d4-4faa40f1db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_baseline(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20723dec-fcf3-49f4-be25-4bffe47aebbf",
   "metadata": {},
   "source": [
    "## Models with Baseline Predictors + Rolling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666a99f4-232b-4c81-b8e6-434e6c74b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_rolling(matches,test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e4ef8-8ee1-4a1e-acd3-50e2aadd5bd3",
   "metadata": {},
   "source": [
    "## Models with Full Set Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68eb9c35-a276-4564-aff3-2e287a5bc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_full(new_matches,new_test_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
