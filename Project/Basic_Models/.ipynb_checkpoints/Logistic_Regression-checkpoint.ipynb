{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e7ba49-f605-4375-bd0b-75b1ace6239c",
   "metadata": {},
   "source": [
    "# Logistic Regression  \n",
    "\n",
    "Logistic Regression is a statistical model used for binary classification problems. It estimates the probability that a given input belongs to a particular class by applying the logistic (sigmoid) function. Unlike linear regression, which predicts continuous values, logistic regression outputs probabilities that are mapped to discrete classes.  \n",
    "\n",
    "#### How It Works:  \n",
    "- **Sigmoid Function**: Converts linear predictions into probabilities using the formula:  \n",
    "  $$\n",
    "P(y=1|X) = \\frac{1}{1 + e^{-(wX + b)}}\n",
    "$$\n",
    "- **Decision Boundary**: If the probability is above a certain threshold (typically 0.5), the instance is classified as one class; otherwise, it belongs to the other class.  \n",
    "- **Cost Function**: Uses log loss (cross-entropy loss) instead of mean squared error to measure the model’s performance.  \n",
    "- **Optimization**: The weights are updated using optimization techniques like **Gradient Descent** to minimize the cost function.  \n",
    "\n",
    "#### Advantages:  \n",
    "✅ **Simple and Interpretable**: Easy to implement and provides clear insights into feature importance.  \n",
    "✅ **Efficient for Binary Classification**: Works well when the target variable has only two classes.  \n",
    "✅ **Probabilistic Output**: Unlike other classification models, it provides class probabilities, making it useful in decision-making tasks.  \n",
    "✅ **Regularization Support**: Can be extended with L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.  \n",
    "\n",
    "#### Disadvantages:  \n",
    "❌ **Limited to Linear Boundaries**: Assumes a linear relationship between features and log-odds, which may not always hold.  \n",
    "❌ **Not Suitable for Complex Data**: Fails to capture non-linear patterns unless combined with feature engineering or kernel tricks.  \n",
    "❌ **Sensitive to Imbalanced Data**: If one class is significantly more frequent than the other, it may bias the model toward the majority class.  \n",
    "❌ **Feature Scaling Required**: Performs better with normalized or standardized input data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69a549-f01d-4e55-a06b-565c8e10be44",
   "metadata": {},
   "source": [
    "### Logistic Regression using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ffaacf-13fd-470f-9803-6305eb56d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_base(Train, Test):\n",
    "    best_C =find_optimal_C_base(Train)  # Function to find the best regularization parameter\n",
    "    \n",
    "    static_predictors =  parameters_base(Train,Test)\n",
    "\n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[static_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b57b15-30f0-4478-a8a6-8bc397c5409f",
   "metadata": {},
   "source": [
    "### Logistic Regression using Baseline Predictors + Rolling Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a9c768-5f91-4e42-8ca4-0fc0f4b6144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_roll(Train, Test):\n",
    "    best_C =find_optimal_C_roll(Train)  # Function to find the best regularization parameter\n",
    "\n",
    "    all_predictors = parameters_roll(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "    \n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[all_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d6b6e-cbcc-4555-a533-f03a80959ba1",
   "metadata": {},
   "source": [
    "### Logistic Regression using Full Feature Set  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19558038-2a3f-4adf-8d2c-06585a337f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_full(Train, Test):\n",
    "    best_C =find_optimal_C_full(Train)  # Function to find the best regularization parameter\n",
    "\n",
    "    all_predictors = parameters_full(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[all_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
