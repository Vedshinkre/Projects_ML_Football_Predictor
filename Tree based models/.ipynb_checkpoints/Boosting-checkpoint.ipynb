{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b0dd3f-fda8-4f3f-bd09-762e5f09b63d",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner. Unlike Bagging and Random Forest, which build trees independently, Boosting trains trees sequentially, where each new tree corrects the errors of the previous ones.\n",
    "\n",
    "#### How It Works:\n",
    "- **Sequential Training**: Trees are trained one after another, with each tree focusing on the misclassified instances of the previous tree.\n",
    "- **Weighted Data**: In each iteration, the incorrectly classified instances are given more weight, so subsequent trees focus more on difficult cases.\n",
    "- **Aggregation**: The final prediction is made by combining the weighted predictions of all trees, typically using a weighted vote (for classification) or weighted average (for regression).\n",
    "\n",
    "#### Advantages:\n",
    "✅ **Reduces Bias**: Boosting improves the accuracy by reducing both bias and variance.  \n",
    "✅ **Highly Accurate**: Often produces better results than individual models due to its focus on correcting errors.  \n",
    "✅ **Works Well with Complex Data**: Can handle complex data distributions and capture subtle patterns in the data.  \n",
    "✅ **Flexible**: Can be applied to a wide range of models, and different base learners (like decision trees, logistic regression, etc.) can be used.\n",
    "\n",
    "#### Disadvantages:\n",
    "❌ **Prone to Overfitting**: If too many trees are added, boosting can overfit the training data, especially if the base learner is too complex.  \n",
    "❌ **Computationally Expensive**: Training sequential trees can be slow and resource-intensive.  \n",
    "❌ **Less Interpretability**: Like Random Forest, boosting ensembles (e.g., Gradient Boosting) can be difficult to interpret.  \n",
    "❌ **Sensitive to Noisy Data**: Boosting can be sensitive to noise in the data, as it places more weight on difficult cases, which might be noisy or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e06e13a-37e8-4258-bc14-ea33d30b3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d6e9f4-1174-4491-ace8-2449016c0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba21d1a5-3afb-432a-b636-759ccfc7d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%run Classification_Tree.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c910e86-2f91-47e6-80f4-0e93e06d7c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee5265d-f727-421b-9358-7e1600e69811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cdf740c-48f3-4bf7-810f-414923f53319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6479c653-3439-49c4-b457-bcf5e9c31b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab729e58-5238-4d06-b9d0-07ed5c348d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec83f8-3eeb-4d8d-a11c-e53cfd8a40e7",
   "metadata": {},
   "source": [
    "### Boosting using Baseline Predictors (refer /Data/Data_Formatting.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ae93c4-0d7e-4e7a-a196-6cad68529d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Gradient Boosting\n",
    "def make_yearly_predictions_gb(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "   # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "\n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "\n",
    "      # Train a Gradient Boosting Classifier\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "           # Predict on test data\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "            \n",
    "             \n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a5648-46e5-4c7e-b28b-32cce5016b86",
   "metadata": {},
   "source": [
    "### Boosting using Baseline Predictors + Rolling Predictors (refer /Data/Data_Formatting.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da18489c-486a-4a6d-a21e-f69b6c932422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\",]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "\n",
    "     # Train a Gradient Boosting Classifier\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1)\n",
    "    gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "           preds = gb_clf.predict(test_year[static_predictors])\n",
    "  \n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d618ffc-8401-4f00-a607-545e6d2e3148",
   "metadata": {},
   "source": [
    "### Boosting using Full Feature Set (refer /Data/Data_Formatting.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd81676f-cd14-4d5a-a332-da9ad4b83b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_gb_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\",]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "\n",
    "     # Train a Gradient Boosting Classifier\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=10, min_samples_split=10, random_state=1)\n",
    "    gb_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = gb_clf.predict(test_year[static_predictors])\n",
    "  \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "           # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Boosting\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcda17-9990-4223-b7b6-cb1d2aead8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
