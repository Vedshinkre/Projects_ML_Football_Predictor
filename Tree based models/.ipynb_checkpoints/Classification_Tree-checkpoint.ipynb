{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52995372-8e13-4670-aa1d-071cfcd1eb1e",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "\n",
    "A **Classification Tree** is a type of decision tree used for classification tasks. It works by recursively splitting the dataset into subsets based on feature values, aiming to maximize the separation between different classes. The final result is a tree-like model where each leaf node represents a class label.\n",
    "\n",
    "## How It Works:\n",
    "- **Recursive Partitioning**: The dataset is split into smaller groups using the most informative features.\n",
    "- **Gini Impurity / Entropy**: The quality of splits is determined using metrics like Gini Impurity or Entropy.\n",
    "- **Tree Growth**: The process continues until a stopping criterion is met (e.g., maximum depth, minimum samples per split).\n",
    "- **Prediction**: For a new input, the model follows the decision path and assigns a class label based on the majority vote in the final node.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Easy to Interpret**: The decision-making process is visual and intuitive.  \n",
    "✅ **Requires Minimal Data Preprocessing**: No need for feature scaling or normalization.  \n",
    "✅ **Captures Non-Linear Relationships**: Works well with complex decision boundaries.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Prone to Overfitting**: Without pruning, the tree can become too complex and fit noise in the data.  \n",
    "❌ **Unstable**: Small changes in data can result in a significantly different tree.  \n",
    "❌ **Less Accurate Than Ensembles**: Single decision trees are often outperformed by ensemble methods like Random Forests and Gradient Boosting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df4dcac-997c-4f87-b3d6-c612bb1f330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17eba866-2df8-4388-b0d7-23f0524c3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e38ebb4-6dba-4705-b00e-34fb464b366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389b2487-8765-4c1f-a2c6-78ea10629255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268b8c1f-b47f-4c85-bb09-3c0c16824d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070733e5-a4bb-43b1-9324-039cbd61f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb02636-ef48-4378-a057-c767f36032a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94448170-ae2a-4ea0-a1aa-ca8d43a98e6c",
   "metadata": {},
   "source": [
    "## **Pruning in Classification Tree**  \n",
    "Pruning helps prevent **overfitting** by reducing the size of a decision tree, leading to improved accuracy on unseen data. Without pruning, a tree may **memorize** training data rather than generalizing well to new data.  \n",
    "\n",
    "### **Post-Pruning (Cost Complexity Pruning - CCP)**  \n",
    "In post-pruning, the tree is first grown to full depth (even if it overfits) and then gradually pruned by removing nodes based on a complexity parameter α .  \n",
    "\n",
    "#### **How CCP Works?**  \n",
    "The pruning process minimizes the following equation:  \n",
    "\n",
    "$$\n",
    "\\text{Total Cost} = \\text{RSS} + \\alpha \\times \\text{Number of Leaves}\n",
    "$$\n",
    "\n",
    "\n",
    "- **RSS (Residual Sum of Squares)** measures the error in predictions.  \n",
    "- **α** is a tuning parameter that controls the trade-off between tree complexity and error.  \n",
    "  - **Higher α** → More pruning → Simpler tree.  \n",
    "  - **Lower α** → Less pruning → More complex tree.  \n",
    "- The value for **α** can be found using cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7e8e64-b5b9-4335-8408-81ad91f926fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find optimal ccp_alpha\n",
    "def find_optimal_alpha(Train):\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    \n",
    "    dt = DecisionTreeClassifier(random_state=1)\n",
    "    path = dt.cost_complexity_pruning_path(Train[static_predictors], Train[\"Target\"])\n",
    "    ccp_alphas = path.ccp_alphas[:-1]  # Exclude the last value to avoid a single-node tree\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    alpha_scores = {}\n",
    "    \n",
    "    for alpha in ccp_alphas:\n",
    "        dt = DecisionTreeClassifier(random_state=1, ccp_alpha=alpha)\n",
    "        scores = cross_val_score(dt, Train[static_predictors], Train[\"Target\"], cv=kf, scoring='accuracy')\n",
    "        alpha_scores[alpha] = np.mean(scores)\n",
    "    \n",
    "    best_alpha = max(alpha_scores, key=alpha_scores.get)\n",
    "    print(f\"Best ccp_alpha: {best_alpha:.6f} with Accuracy: {alpha_scores[best_alpha]:.4f}\")\n",
    "    return best_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5841372-4a4e-4cf8-a222-f638fefa8ed9",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7cefd0a-68ed-4ab2-88f0-f9bdeb2a6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions\n",
    "def make_yearly_predictions_decs(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "\n",
    "     # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[static_predictors], Train[\"Target\"])\n",
    "  \n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "           preds = dt.predict(test_year[static_predictors])\n",
    "           \n",
    "             # Calculate precision and accuracy\n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "           # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89b71f-69f6-452c-8593-18435bd6e6ff",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Baseline Predictors + Rolling Predictors (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a44b77-a186-4593-a4b9-28d306cddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "\n",
    "    # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "           preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "             # Calculate precision and accuracy\n",
    "           precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "           accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "              # Append results to list\n",
    "           results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "           })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4cec5-bacb-4484-b6b2-099df8697bf0",
   "metadata": {},
   "source": [
    "### Classifiaction Tree using Full Feature Set (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3275c3c1-dc5f-4ffe-a0cc-ef741851daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_decs_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors =  [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "     # Train Decision Tree with externally provided ccp_alpha\n",
    "    dt = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    dt.fit(Train[all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = dt.predict(test_year[all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Classification Tree\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return (results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bde66-110c-4356-93fa-45cf6858700c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
