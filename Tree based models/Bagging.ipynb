{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f90f998-f046-4278-985e-34e016a11615",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging is an ensemble learning technique where multiple models (typically decision trees) are trained independently on different random subsets of the data, and their predictions are aggregated to make a final decision. \n",
    "\n",
    "## How It Works:\n",
    "- **Bootstrap Sampling**: Random subsets of data are sampled with replacement.\n",
    "- **Parallel Training**: Each model is trained independently on its own subset of data.\n",
    "- **Aggregation**: The results from all models are combined, often by majority voting (classification) or averaging (regression).\n",
    "- **Reduces Variance**: Bagging helps make the model more stable and less sensitive to fluctuations in the data.\n",
    "\n",
    "## Advantages:\n",
    "✅ **Reduces Overfitting**: Combining multiple models reduces the likelihood of overfitting.  \n",
    "✅ **Improves Accuracy**: Bagging tends to improve accuracy compared to individual models.  \n",
    "✅ **Stable Predictions**: By aggregating predictions, the model becomes more robust.  \n",
    "\n",
    "## Disadvantages:\n",
    "❌ **Increased Computational Cost**: Bagging requires training multiple models, which can be computationally expensive.  \n",
    "❌ **Less Interpretability**: As it uses an ensemble of models, bagging may be less interpretable compared to single decision trees.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98be45d-8aad-4444-aa98-8ca1e108ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b283dd3b-d2f7-42bc-b48b-e3aaa1fc2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb98051-6184-46a6-9b7c-7de9814b79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%run Classification_Tree.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d20affa-a1e3-4766-ac37-ea5858c5b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375a16d8-c4a1-4fe8-8aac-c58b90a5d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3715574b-17e7-4f4c-9ae5-8a925fcab6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "070d7f68-ffdd-4446-bb99-4d2d167218e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facf2302-3227-4779-8747-591fa1a4bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72159e72-0952-4f87-b924-df2523429d1a",
   "metadata": {},
   "source": [
    "### Bagging using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0465ebe0-2d65-49b5-a299-112946853395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    \n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "\n",
    "    # Define static predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "\n",
    "    # Train Bagging model on training data\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1)\n",
    "    bagging_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = accuracy_score(Train['Target'], bagging_clf.predict(Train[static_predictors]))\n",
    "\n",
    "    # Create a list to store results\n",
    "    results = []\n",
    "\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = bagging_clf.predict(test_year[static_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318d8d6-90b8-4fdd-90f1-9d68b65b42a9",
   "metadata": {},
   "source": [
    "### Bagging using Baseline Predictors + Rolling Predictors   (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4eea925-0472-4554-a759-2af637fd33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_rolling(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "\n",
    "    # Train a Bagging Classifier with multiple Decision Trees\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "    bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:  \n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "             # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33b033-214b-4c35-8dfa-9c483d4abc4c",
   "metadata": {},
   "source": [
    "### Bagging using  Full Feature Set (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6e6e73-d238-46c3-b549-0dc7f786ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearly_predictions_bagging_full(Train, Test):\n",
    "    best_alpha = find_optimal_alpha(Train)\n",
    "    # Convert 'Date' columns to datetime and sort data\n",
    "    Train['Date'] = pd.to_datetime(Train['Date'], errors='coerce')\n",
    "    Test['Date'] = pd.to_datetime(Test['Date'], errors='coerce')\n",
    "    Train = Train.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    Test = Test.dropna(subset=['Date']).sort_values(by='Date')\n",
    "    \n",
    "    # Define the feature columns for which we'll calculate rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\", \"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "    \n",
    "    # Apply rolling averages to both Train and Test datasets\n",
    "    train_results = []\n",
    "    for team, group in Train.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        train_results.append(result)\n",
    "    Train = pd.concat(train_results)\n",
    "    \n",
    "    test_results = []\n",
    "    for team, group in Test.groupby(\"Team\"):\n",
    "        result = rolling_averages(group, cols, new_cols)\n",
    "        test_results.append(result)\n",
    "    Test = pd.concat(test_results)\n",
    "    \n",
    "    # Define static and rolling predictors\n",
    "    static_predictors = [\"Venue_code\", \"Opp_code\", \"Hour\", \"Day_code\",\"Rank\",\"IsRanked\"]\n",
    "    rolling_predictors = new_cols\n",
    "    all_predictors = static_predictors + rolling_predictors\n",
    "    \n",
    "    # Train a Bagging Classifier with multiple Decision Trees\n",
    "    base_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=10, ccp_alpha=best_alpha, random_state=1)\n",
    "    bagging_clf = BaggingClassifier(estimator=base_tree, n_estimators=50, random_state=1, n_jobs=-1) \n",
    "    bagging_clf.fit(Train[ all_predictors], Train[\"Target\"])\n",
    "    \n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            preds = bagging_clf.predict(test_year[ all_predictors])\n",
    "            \n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\")\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "            \n",
    "           # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Bagging\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b40a6f-dd30-4c17-b25e-0f4a7d2957ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
