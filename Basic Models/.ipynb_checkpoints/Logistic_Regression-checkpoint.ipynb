{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e7ba49-f605-4375-bd0b-75b1ace6239c",
   "metadata": {},
   "source": [
    "# Logistic Regression  \n",
    "\n",
    "Logistic Regression is a statistical model used for binary classification problems. It estimates the probability that a given input belongs to a particular class by applying the logistic (sigmoid) function. Unlike linear regression, which predicts continuous values, logistic regression outputs probabilities that are mapped to discrete classes.  \n",
    "\n",
    "#### How It Works:  \n",
    "- **Sigmoid Function**: Converts linear predictions into probabilities using the formula:  \n",
    "  $$\n",
    "P(y=1|X) = \\frac{1}{1 + e^{-(wX + b)}}\n",
    "$$\n",
    "- **Decision Boundary**: If the probability is above a certain threshold (typically 0.5), the instance is classified as one class; otherwise, it belongs to the other class.  \n",
    "- **Cost Function**: Uses log loss (cross-entropy loss) instead of mean squared error to measure the model’s performance.  \n",
    "- **Optimization**: The weights are updated using optimization techniques like **Gradient Descent** to minimize the cost function.  \n",
    "\n",
    "#### Advantages:  \n",
    "✅ **Simple and Interpretable**: Easy to implement and provides clear insights into feature importance.  \n",
    "✅ **Efficient for Binary Classification**: Works well when the target variable has only two classes.  \n",
    "✅ **Probabilistic Output**: Unlike other classification models, it provides class probabilities, making it useful in decision-making tasks.  \n",
    "✅ **Regularization Support**: Can be extended with L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.  \n",
    "\n",
    "#### Disadvantages:  \n",
    "❌ **Limited to Linear Boundaries**: Assumes a linear relationship between features and log-odds, which may not always hold.  \n",
    "❌ **Not Suitable for Complex Data**: Fails to capture non-linear patterns unless combined with feature engineering or kernel tricks.  \n",
    "❌ **Sensitive to Imbalanced Data**: If one class is significantly more frequent than the other, it may bias the model toward the majority class.  \n",
    "❌ **Feature Scaling Required**: Performs better with normalized or standardized input data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af91e751-0c86-4ce4-84c4-8d1133f014d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading all the necesaary dependecies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fd9468-89b7-4178-9a9b-64afe43e1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Data_Formatting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6933d26f-ae41-4882-bfbc-071d5ac59dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Ultimate_Hyperparameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbce3c0-9a0d-4a24-ab76-7df2bef97034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Data/Parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c911afa2-0058-4e58-bc2a-bb6c9a0e8d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vedsh\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2696ac-5580-47a4-bd65-a1c397168cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset \n",
    "train_path = Path(\"../Data/premierleague_team_data.csv\")\n",
    "matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data \n",
    "test_path = Path(\"../Data/premierleague_test_team_data.csv\")\n",
    "test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2007511c-9a57-4b26-8b1c-3fd7981564fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training dataset with rank\n",
    "train_path = Path(\"../Data/premierleague_rank_team_data.csv\")\n",
    "new_matches = pd.read_csv(train_path)\n",
    "\n",
    "#loading the testing data with rank\n",
    "test_path = Path(\"../Data/premierleague_rank_test_team_data.csv\")\n",
    "new_test_matches = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec59cc6-632f-466d-b182-668c92070178",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(matches, test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa564a8-75a7-410a-925c-71885faa1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(new_matches, new_test_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69a549-f01d-4e55-a06b-565c8e10be44",
   "metadata": {},
   "source": [
    "### Logistic Regression using Baseline Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ffaacf-13fd-470f-9803-6305eb56d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_base(Train, Test):\n",
    "    best_C =find_optimal_C_base(Train)  # Function to find the best regularization parameter\n",
    "    \n",
    "    static_predictors =  parameters_base(Train,Test)\n",
    "\n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[static_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[static_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b57b15-30f0-4478-a8a6-8bc397c5409f",
   "metadata": {},
   "source": [
    "### Logistic Regression using Baseline Predictors + Rolling Predictors  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a9c768-5f91-4e42-8ca4-0fc0f4b6144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_roll(Train, Test):\n",
    "    best_C =find_optimal_C_roll(Train)  # Function to find the best regularization parameter\n",
    "\n",
    "    all_predictors = parameters_roll(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "    \n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[all_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d6b6e-cbcc-4555-a533-f03a80959ba1",
   "metadata": {},
   "source": [
    "### Logistic Regression using Full Feature Set  (refer /Data/Data_Formatting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19558038-2a3f-4adf-8d2c-06585a337f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make yearly predictions using Logistic Regression\n",
    "def make_yearly_predictions_lr_full(Train, Test):\n",
    "    best_C =find_optimal_C_full(Train)  # Function to find the best regularization parameter\n",
    "\n",
    "    all_predictors = parameters_full(Train,Test)\n",
    "    Train = roll(Train)\n",
    "    Test  = roll(Test)\n",
    "\n",
    "    # Train a Logistic Regression model\n",
    "    lr_clf = LogisticRegression(C=best_C, solver='liblinear', random_state=1 , class_weight='balanced')\n",
    "    lr_clf.fit(Train[all_predictors], Train[\"Target\"])\n",
    "\n",
    "    results = []\n",
    "    for year in range(Test['Date'].dt.year.min(), Test['Date'].dt.year.max() + 1):\n",
    "        test_year = Test[Test['Date'].dt.year == year]\n",
    "        if not test_year.empty:\n",
    "            # Predict on test data\n",
    "            preds = lr_clf.predict(test_year[all_predictors])\n",
    "\n",
    "            # Calculate precision and accuracy\n",
    "            precision = precision_score(test_year[\"Target\"], preds, average=\"weighted\", zero_division=1)\n",
    "            accuracy = accuracy_score(test_year[\"Target\"], preds)\n",
    "\n",
    "            # Append results to list\n",
    "            results.append({\n",
    "                \"Model\": \"Logistic Regression\",\n",
    "                \"Year\": year,\n",
    "                \"Precision\": precision,\n",
    "                \"Accuracy\": accuracy\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
